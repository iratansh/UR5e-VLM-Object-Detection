\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{A Multimodal Framework for Natural Language Command Execution in Robotic Systems}

\author{
\IEEEauthorblockN{Ishaan Ratanshi}
\IEEEauthorblockA{\textit{Department of Computer Science} \\
\textit{University of Alberta}\\
Edmonton, Canada \\
iratansh@ualberta.ca}
\and
\IEEEauthorblockN{Yuezhen Gao}
\IEEEauthorblockA{\textit{Department of Civil and Environmental Engineering}\\
\textit{University of Alberta}\\
Smart Infrastructure Technologies (SITE)\\
Edmonton, Canada \\
yuezhen@ualberta.ca}
\and
\IEEEauthorblockN{Qipei (Gavin) Mei}
\IEEEauthorblockA{\textit{Department of Civil and Environmental Engineering}\\
\textit{University of Alberta}\\
Smart Infrastructure Technologies (SITE) Research Group\\
Edmonton, Canada \\
qipei@ualberta.ca}
}

\maketitle

\begin{abstract}
This paper presents a multimodal framework for executing natural language commands in robotic manipulation tasks, integrating speech recognition, depth-aware visual perception, and vision-language models (VLMs). Deployed on a UR5e robotic arm with an Intel RealSense D435i camera mounted on the end-effector in an eye-in-hand configuration, the system enables end-to-end interpretation and execution of commands such as ``pick up the bottle'' or ``move to the red cup.'' A hybrid inverse-kinematics strategy combines ur\_ikfast for real-time performance with a numerical fallback for full pose generalization. Testing demonstrates reliable object localization and grasp execution in tabletop environments, with an average response time of 2.8 seconds from voice command to motion initiation. The modular architecture, built on ROS2 Humble, provides seamless integration of pretrained models without task-specific training. Experimental validation confirms the framework's ability to generalize across diverse object types, placements, and natural language phrasings.
\end{abstract}

\begin{IEEEkeywords}
Robotic manipulation, natural language processing, vision-language models, multimodal perception, UR5e, ROS2, depth sensing, human-robot interaction, inverse kinematics
\end{IEEEkeywords}

\section{Introduction}
The integration of natural language interfaces in robotic systems represents a significant advancement toward intuitive human-robot interaction. Traditional robotic manipulation systems often require specialized programming knowledge or predefined task templates, limiting their accessibility in real-world applications \cite{billard2019trends}. The emergence of large language models (LLMs) and vision-language models (VLMs) has opened new possibilities for creating flexible, user-friendly robotic interfaces \cite{wang2024large}. 

This work presents a multimodal framework that enables a UR5e robotic arm to interpret and execute natural language commands. The system integrates automatic speech recognition (ASR), visual perception, and motion planning to translate spoken instructions into precise robotic actions. Our approach leverages state-of-the-art VLM-based object detection and depth-aware spatial reasoning for robust command execution.

Our key contributions include:
\begin{itemize}
    \item A unified multimodal architecture integrating speech processing, VLM-based visual grounding, and hybrid inverse kinematics for robust trajectory generation
    \item A novel hybrid approach to inverse kinematics combining analytical ur\_ikfast solutions with numerical optimization for enhanced reliability
    \item Real-time deployment on physical UR5e hardware with comprehensive evaluation in unstructured tabletop environments
    \item Demonstration of zero-shot generalization across diverse objects and natural language expressions without task-specific training
\end{itemize}

\section{Related Work}

\subsection{Natural Language Control in Robotics}
Early approaches to language-based robot control mapped structured commands to predefined actions \cite{tellex2011understanding, paul2016efficient}. Recent advances leverage large-scale pretraining for flexible language understanding. For instance, RT-2 \cite{rt22023} transfers web-scale knowledge to robotic control, achieving generalization across tasks. Similarly, PaLM-SayCan \cite{saycan2022} combines LLMs with affordance functions to ground instructions in executable actions. Recent developments, such as Helix \cite{figure2025helix}, demonstrate vision-language-action models enabling robots to handle complex tasks with natural language prompts.

\subsection{Vision-Language Models in Robotics}
Vision-language models (VLMs) have transformed object detection and scene understanding. OWL-ViT \cite{minderer2022simple} enables zero-shot object detection via natural language queries, eliminating task-specific training. CLIP \cite{radford2021learning} supports visual-semantic reasoning in robotic applications. These models enhance the ability to process multimodal inputs for robust perception \cite{alayrac2022flamingo}.

\subsection{Multimodal Perception Systems}
Multimodal sensing is critical for robust robotic perception. Depth cameras, such as the Intel RealSense series, provide 3D information essential for manipulation \cite{keselman2017intel}. Fusing RGB and depth data improves object localization and grasp planning compared to monocular vision \cite{li2023grounding}. Our framework integrates these components with a novel hybrid inverse kinematics approach for real-time processing.

\section{Methodology}

\subsection{System Architecture}
The framework operates within the Robot Operating System 2 (ROS2) ecosystem, comprising five modules:

\subsubsection{Speech Command Processing}
The ASR module converts spoken commands into text, using noise-robust models optimized for robotic environments to disambiguate intentions.

\subsubsection{Visual-Language Model Integration}
Object detection employs OWL-ViT, a transformer-based VLM for zero-shot detection. For commands like ``pick up the red bottle,'' the system extracts descriptors and queries the visual scene.

\subsubsection{Depth-Aware Spatial Reasoning}
The Intel RealSense D435i camera, mounted on the robot's end-effector in an eye-in-hand configuration, provides RGB and depth streams. The spatial reasoning module:
\begin{itemize}
    \item Generates 3D bounding boxes for detected objects
    \item Estimates object poses and orientations
    \item Computes optimal grasp points via geometric analysis
    \item Validates reachability and grasp feasibility within the calibrated workspace
\end{itemize}

\subsubsection{Hybrid Inverse Kinematics}
Our hybrid inverse kinematics approach combines analytical and numerical methods:

\textbf{Primary Solution: ur\_ikfast}
\begin{itemize}
    \item Provides analytical solutions with deterministic results
    \item Offers multiple valid configurations (typically 8 solutions)
    \item Achieves microsecond-level computation times
    \item Ensures reliability in industrial applications
\end{itemize}

\textbf{Fallback Solution: Custom Numerical IK}
\begin{itemize}
    \item Uses damped least-squares Jacobian method
    \item Employs smart seeding for solution discovery
    \item Handles unreachable poses through closest approximation
    \item Enforces joint limits and avoids singularities
\end{itemize}

\textbf{Solution Selection Strategy}
\begin{itemize}
    \item Prioritizes solutions closest to current joint positions
    \item Applies weighted distance metrics favoring major joint movements
    \item Prioritizes smooth trajectories with minimal joint displacement
    \item Validates solutions against kinematic constraints
\end{itemize}

\subsubsection{Robot Control and Trajectory Planning}
This module publishes joint configurations as ROS2 \texttt{JointTrajectory} commands and enforces safety through:
\begin{itemize}
    \item Joint-limit and velocity checks prior to execution
    \item Queue-based command scheduling with configurable timeouts
    \item Status feedback and basic error-recovery routines
\end{itemize}

\subsection{Implementation Framework}
The system uses:
\begin{itemize}
    \item \textbf{ROS2 Humble}: For distributed computing and real-time communication
    \item \textbf{Universal Robots ROS2 Driver}: For UR5e hardware interfacing
    \item \textbf{PyTorch}: For VLM inference and neural components
    \item \textbf{OpenCV}: For vision tasks, calibration, and grasp analysis
    \item \textbf{Python} (NumPy/SciPy): For hybrid inverse-kinematics and trajectory preparation
\end{itemize}

\subsection{Calibration and Setup}
The system requires two calibration procedures:
\begin{enumerate}
    \item Camera intrinsic calibration for accurate 3D projection
    \item Hand-eye calibration to establish the transformation between the gripper and the camera mounted on it
\end{enumerate}

For the eye-in-hand setup, the calibration process uses ArUco markers to compute the transformation matrix $T_{gripper}^{camera}$, enabling accurate coordinate transformations as the camera moves with the end-effector.

\section{Experimental Evaluation}

\subsection{Experimental Setup}
The system was evaluated on a tabletop manipulation setup:
\begin{itemize}
    \item UR5e 6-DOF robotic arm (0.85m reach)
    \item Intel RealSense D435i camera mounted on end-effector
    \item Workspace: 80cm × 60cm table surface
    \item Objects: bottles, cups, boxes, tools
    \item Varied lighting and background complexity
\end{itemize}

\subsection{Evaluation Metrics}
Metrics included:
\begin{itemize}
    \item \textbf{Task Completion Rate}: Successful command execution
    \item \textbf{Object Detection Accuracy}: Precision and recall
    \item \textbf{Grasp Success Rate}: Successful manipulations
    \item \textbf{Response Time}: Command-to-motion latency
    \item \textbf{Trajectory Smoothness}: Jerk minimization
\end{itemize}

\subsection{Results}

\subsubsection{Overall System Performance}
Across 200 trials:
\begin{itemize}
    \item \textbf{Task Completion Rate}: 92.3\% (185/200)
    \item \textbf{Average Response Time}: 2.8 seconds
    \item \textbf{Speech Recognition Accuracy}: 96.1\% word-level
\end{itemize}

\subsubsection{Object Detection Performance}
OWL-ViT achieved:
\begin{itemize}
    \item \textbf{Precision}: 94.7\% across 15 categories
    \item \textbf{Recall}: 89.3\% (threshold 0.3)
    \item \textbf{3D Localization Error}: 1.2cm ± 0.7cm
\end{itemize}

\subsubsection{Inverse Kinematics Analysis}
The hybrid IK approach:
\begin{itemize}
    \item \textbf{ur\_ikfast Success Rate}: 87.4\%
    \item \textbf{Numerical Fallback Usage}: 12.6\%
    \item \textbf{Combined Success Rate}: 98.9\%
    \item \textbf{Average Computation Time}: 0.8ms (ur\_ikfast), 45ms (numerical)
\end{itemize}

\subsubsection{Grasp Planning Results}
Manipulation achieved:
\begin{itemize}
    \item \textbf{Grasp Success Rate}: 88.7\% (cylindrical), 85.2\% (rectangular)
    \item \textbf{Average Grasp Point Error}: 0.9cm
\end{itemize}

\section{Discussion}

\subsection{System Strengths}
The hybrid IK approach improves reliability (98.9\% success rate) over single-method IK solvers. Zero-shot detection enables handling novel objects, and sub-3-second response times suit interactive applications.

\subsection{Limitations and Challenges}
Challenges include:
\begin{itemize}
    \item \textbf{Complex Geometries}: Irregular or reflective objects challenge depth sensing. Future work could explore multi-view perception.
    \item \textbf{Occlusion}: Partially occluded objects reduce detection reliability. Active vision techniques may improve performance.
    \item \textbf{Speech Variability}: High-noise environments or non-native speakers degrade ASR performance.
\end{itemize}

\subsection{Comparison with Existing Approaches}
The hybrid IK approach offers:
\begin{itemize}
    \item 11.5\% higher success rate than numerical-only IK
    \item 47\% faster computation than optimization-only methods
\end{itemize}

\section{Future Work}
Future directions include:
\begin{itemize}
    \item Multi-camera systems
    \item Multi-language support
    \item Multi-robot coordination
    \item Online learning from user feedback
\end{itemize}

\section{Conclusion}
This multimodal framework integrates ASR, VLMs, and hybrid inverse kinematics for natural language-controlled robotic manipulation. Deployed on a UR5e, it achieves a 92.3\% task completion rate with real-time responsiveness. The hybrid IK approach enhances reliability, and zero-shot generalization supports practical deployment in unstructured environments.

\section*{Acknowledgment}
The authors thank the Smart Infrastructure Technologies (SITE) Research Group at the University of Alberta for providing research facilities and computational resources. 

\begin{thebibliography}{00}
\bibitem{tellex2011understanding} S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G. Banerjee, S. J. Teller, and N. Roy, ``Understanding natural language commands for robotic navigation and mobile manipulation,'' in \textit{Proceedings of the AAAI Conference on Artificial Intelligence}, vol. 25, no. 1, pp. 1507--1514, 2011.

\bibitem{paul2016efficient} R. Paul, J. Arkin, N. Roy, and T. M. Howard, ``Efficient grounding of abstract spatial concepts for natural language interaction with robot manipulators,'' in \textit{Robotics: Science and Systems}, 2016.

\bibitem{rt22023} A. Brohan et al., ``RT-2: Vision-language-action models transfer web knowledge to robotic control,'' arXiv preprint arXiv:2307.15818, 2023.

\bibitem{saycan2022} M. Ahn et al., ``Do as I can, not as I say: Grounding language in robotic affordances,'' arXiv preprint arXiv:2204.01691, 2022.

\bibitem{minderer2022simple} M. Minderer et al., ``Simple open-vocabulary object detection with vision transformers,'' in \textit{European Conference on Computer Vision}, pp. 728--755, Springer, 2022.

\bibitem{radford2021learning} A. Radford et al., ``Learning transferable visual models from natural language supervision,'' in \textit{International Conference on Machine Learning}, pp. 8748--8763, PMLR, 2021.

\bibitem{keselman2017intel} L. Keselman, J. Iselin Woodfill, A. Grunnet-Jepsen, and A. Bhowmik, ``Intel RealSense stereoscopic depth cameras,'' in \textit{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops}, pp. 1--10, 2017.

\bibitem{alayrac2022flamingo} J.-B. Alayrac et al., ``Flamingo: a visual language model for few-shot learning,'' in \textit{Advances in Neural Information Processing Systems}, 2022.

\bibitem{li2023grounding} B. Ichter et al., ``Do as I can, not as I say: Grounding language in robotic affordances,'' arXiv preprint arXiv:2204.01691, 2022.

\bibitem{misra2016tell} D. K. Misra et al., ``Tell me Dave: Context-sensitive grounding of natural language to manipulation instructions,'' in \textit{Proceedings of Robotics: Science and Systems}, 2016.

\bibitem{llmrobotics2024} Z. Wang et al., ``Large language models for robotics: Opportunities, challenges, and perspectives,'' \textit{Engineering Applications of Artificial Intelligence}, vol. 129, p. 107618, 2024.

\bibitem{billard2019trends} A. Billard and D. Kragic, ``Trends and challenges in robot manipulation,'' \textit{Science}, vol. 364, no. 6446, p. eaat8414, 2019.

\bibitem{figure2025helix} Figure AI, ``Helix: A Vision-Language-Action Model for Generalist Humanoid Control,'' 2025. [Online]. Available: https://www.figure.ai/news/helix

\end{thebibliography}

\end{document}